---
title:  "[딥러닝을 위한 선형대수학] 1.1 행렬 A의 열을 이용한 곱셈"
excerpt: "행렬 A의 연산을 열의 관점으로 알아봅시다."

categories: [Study Notes, Linear Algebra]
tags: [Linear Algebra]

toc: true
toc_sticky: true

date: 2023-08-05
last_modified_at: 2025-04-19

math: true
---

# Chapter 1.1 행렬 $A$의 열을 이용한 곱셈 $Ax$

## 주요 키워드

`일차결합(Linear combination)` `열공간(Column space)` `Rank` `기저(Basis)` `일차독립(Linear independence)`

## Summary

### $Ax=b$

- 행렬 $A$의 행(각각이 벡터임)과 $x$의 내적
- 행렬 $A$의 열(각각이 벡터임)의 일차결합

    > **일차결합(선형결합, linear combination)**
    >
    > 벡터의 덧셈과 스칼라곱을 조합해 새로운 벡터를 얻는 연산

    - $Ax=b$를 만족하는 해 $x$가 존재하지 않음 $\Leftrightarrow$ 벡터 $b$를 행렬 $A$의 일차결합으로 만들 수 없음 $\Leftrightarrow$ $b \notin C(A)$ (벡터 $b$는 행렬 $A$의 열공간 $C(A)$에 속하지 않음)
        
        > **열공간(Column space)**
        > 
        > 
        > 행렬 $A$의 열공간 $C(A)$는 행렬 $A$의 열의 일차결합으로 만들어진 모든 벡터를 원소로 가집니다.
        > 
        > 행렬 $A$의 열의 일차결합은 $A$의 열공간을 모두 채웁니다.
        > 


### 열공간의 차원(dimension), 독립인 열, 랭크(rank)

- **열공간의 차원 $dim(C(A))$
= 행렬 $A$의 일차독립인 열의 개수
= 열공간 $C(A)$의 기저벡터 개수
= 행렬 $A$의 랭크 $r$
= 행공간의 차원 $dim(C(A^T))$**

    > **일차독립(선형독립, linear independence)**
    > 
    > 
    > 벡터들의 집합 $\{ v_1 , v_2, \cdots , x_n \}$과 스칼라 $c_1, c_2, \cdots , c_n$에 대해, 일차결합 $c_1 v_1 + c_2 v_2 + \cdots + c_n v_n=0$을 만족하는 경우가
    > 
    > - $c_1 , c_2, \cdots, c_n$가 0인 경우 외에는 없는 경우, 일차독립
    > - 0이 아닌 $c$가 하나라도 존재하는 일차결합으로 만족시킬 수 있다면, 선형종속

    > **기저(Basis)**
    > 
    > 
    > 벡터공간 $V$의 기저는
    > 
    > 1. 일차독립이고
    > 2. 벡터공간 $V$를 span합니다.
    > 
    > > **Span**
    > > 
    > > 
    > > 벡터 $v_1, v_2, \cdots, v_n$의 모든 일차결합으로 어떠한 벡터공간을 형성(span a space)합니다.
    > > 
    > 
    > ⇒ 기저로 벡터공간 $V$의 모든 벡터를 표현할 수 있음
    > 
    > ⇒ **기저는 벡터공간 $V$를 만들기 위한 최소한의 벡터**
    > 

    ⚠️ 기저는 다양할 수 있지만, 랭크는 항상 일정합니다.

### $A=CR$

행렬 $A$를 행렬 $A$의 열공간 $C(A)$의 기저를 열로 하는 행렬 $C$와 행공간 $C(A^T)$의 기저를 행으로 하는 $R$로 분해할 수 있습니다.

$$
A =
\begin{bmatrix}
1 & 3 & 8 \\
1 & 2 & 6 \\
0 & 1 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 3 \\
1 & 2 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 2 \\
0 & 1 & 2
\end{bmatrix}
= CR
$$

![A=CR을 열 관점으로 보기](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/A=CR%201.png)

**행렬 $C$는 행렬 $A$의 열공간 $C(A)$의 기저**입니다. 
**행렬 $R$의 각 열은 행렬 $A$의 각 열을 열공간의 기저의 일차결합으로 표현할 때의 상수**를 담고 있습니다.

![A=CR을 행 관점으로 보기](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/A=CR%202.png)

**행렬 $R$은 행렬 $A$의 행공간 $C(A^T)$의 기저**입니다. 
**행렬 $C$의 각 행은 행렬 $A$의 각 행을 행공간의 기저의 일차결합으로 표현할 때의 상수**를 담고 있습니다.

---

## 본문

### **열 관점으로 $Ax$ 바라보기**


✏️ **예제 1**

행렬 $A =
\begin{bmatrix}
2 & 3 \\
2 & 4 \\
3 & 7
\end{bmatrix}$, 벡터 $x= \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}$에 대해, $Ax$ 계산하기

1. 행(row) 중심: 행과 $x$의 내적(dot product)으로 표현하기
    
    $$
    \begin{bmatrix}
    2 & 3 \\
    2 & 4 \\
    3 & 7
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2
    \end{bmatrix}
    =
    \begin{bmatrix}
    2x_1 + 3x_2 \\
    2x_1 + 4x_2 \\
    3x_1 + 7x_2
    \end{bmatrix}
    $$
    
2. ⭐ 열(column) 중심: 행렬 $A$의 **열의 일차결합**(선형결합, linear combination)으로 표현하기

    > 일차결합(선형결합, linear combination)
    > 
    > 벡터 $v_1 , v_2, \cdots , v_n$과 스칼라 $c_1, c_2, \cdots , c_n$에 대해, $c_1 v_1 + c_2 v_2 + \cdots + c_n v_n$ 과 같은 표현식을 벡터 $v_1 , v_2, \cdots , v_n$의 일차결합이라고 합니다.
    
    $$
    \begin{bmatrix}
    2 & 3 \\
    2 & 4 \\
    3 & 7
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2
    \end{bmatrix}
    =
    x_1 \begin{bmatrix}
    2 \\
    2 \\
    3
    \end{bmatrix}
    +
    x_2 \begin{bmatrix}
    3 \\
    4 \\
    7
    \end{bmatrix}
    $$

🤔1. 열과 해의 일차결합이라는 것이 의미하는 것은 무엇일까요?

> **벡터공간(Vector space)**
> 
> 벡터공간 $V$의 원소 벡터 $u$, $v$는 아래 조건을 만족합니다.
> 
> 1. 벡터끼리의 덧셈에 대해 닫혀있음: $u + v \in V$
> 2. 스칼라곱에 대해 닫혀있음: $ku \in V$
> 3. 영벡터가 반드시 벡터공간 $V$의 원소여야 함

벡터공간의 원소인 벡터를 가지고 하는 벡터의 덧셈과 스칼라곱으로 만들어진 새로운 벡터 역시 벡터공간의 원소입니다.

행렬 $A$의 열들과 임의의 벡터 $x$를 가지고 일차결합을 수행해, 행렬 $A$의 **열들과 같은 벡터공간에 있는 무수히 많은 벡터** $Ax$를 구할 수 있습니다.

> **열공간(Column space)**
> 
> 행렬 $A$의 열공간 $C(A)$는 행렬 $A$의 열의 일차결합으로 만들어진 모든 벡터를 원소로 가집니다.
> 
> 행렬 $A$의 **열의 일차결합은 $A$의 열공간을 모두 채웁니다**.

벡터 $b$가 행렬 $A$의 열의 일차결합으로 표현할 수 있도록 하는 스칼라 $x_1, \cdots, x_n$의 모임 $x$가

- 존재한다면, 벡터 $b$는 행렬 $A$의 열공간 $C(A)$의 원소입니다.
- 존재하지 않는다면(= 벡터 $b$를 일차결합으로 표현할 수 없다면) $b$는 행렬 $A$의 열공간 $C(A)$의 원소가 아닙니다.


💡1. 열의 관점에서, **해 $x$는 벡터 $b$를 행렬 $A$의 열의 일차결합으로 어떻게 표현할 수 있는지**를 나타냅니다. 해가 존재한다면 $b$는 $C(A)$의 원소입니다.

### 열공간의 차원, 독립인 열, 랭크


💡2. **열공간의 차원은 서로 독립인 벡터가 몇 개인지**에 따라 결정됩니다.

> **일차독립(선형독립, linear independence)**
> 
> 벡터들의 집합 $\{ v_1 , v_2, \cdots , x_n \}$과 스칼라 $c_1, c_2, \cdots , c_n$에 대해, 일차결합 $c_1 v_1 + c_2 v_2 + \cdots + c_n v_n=0$을 만족하는 경우가
> 
> - **$c_1 , c_2, \cdots, c_n$가 0인 경우 외에는 없는 경우 일차독립**
> - 0이 아닌 $c$가 하나라도 존재하는 일차결합으로 만족시킬 수 있다면 선형종속

행렬 $A =
\begin{bmatrix}
2 & 3 \\
2 & 4 \\
3 & 7
\end{bmatrix}$에 대해,

- 두 열 $a_1$, $a_2$가 모두 일차독립이고,
- 일차결합 $Ax =
\begin{bmatrix}
2x_1 + 3x_2 \\
2x_1 + 4x_2 \\
3x_1 + 7x_2
\end{bmatrix}
=
x_1 \begin{bmatrix}
2 \\
2 \\
3
\end{bmatrix} +
x_2 \begin{bmatrix}
3 \\
4 \\
7
\end{bmatrix}$는 $R^3$ 위의 3차원 벡터이고,
- 행렬 $A$의 각 열 $a_1$, $a_2$에 대해, 모든 일차결합 $Ax = x_1 a_1 + x_2 a_2$로 채워진 열공간 $C(A)$는 평면으로, $R^3$를 모두 채우지 않습니다.

![열공간 시각화](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/C(A)%20visualization.png){: width="50%" height="50%"}

> 행렬 $A$의 두 열벡터(노랑)와 열공간 $C(A)$(짙은 회색)

행렬 $A$에는 일차종속인 열들이 존재할 수 있습니다. 열공간을 만드는 최소한의 벡터가 무엇인지, 열공간의 차원은 얼마일지 파악하고자 할 때 용이할, 일차독립인 벡터들의 집합만 담은 행렬 $C$를 만들어봅시다.


#### 행렬 $A$로부터 열이 일차독립인 행렬 $C$ 만들기

1. 행렬 $A$의 1열이 영벡터가 아니면, 행렬 $C$에 포함하기
2. 행렬 $A$의 2열이 1열의 상수를 곱한 것과 같지 않으면, 행렬 $C$에 포함하기
3. 행렬 $A$의 3열이 1열과 2열의 일차결합이 아니면, 이를 행렬 $C$에 포함하기
    
    > **일차독립 변형하기**
    > 
    > 
    > $c_1 v_1 + c_2 v_2 + \cdots + c_k v_k=0$를 벡터 $v_k = -\frac{c_1}{c_k}v_1 -\frac{c_2}{c_k}v_2 - \cdots - \frac{c_{k-1}}{c_k} v_{k-1}$의 형태로 변형할 수 있습니다.
    > 
    > 벡터 $v_k$를 다른 벡터들의 일차결합으로 나타낼 수 있는지의 문제로 바꿔 생각할 수 있습니다!
4. 행렬 $A$의 모든 $n$개의 열에 대해 위 작업 반복 수행하기
5. 열이 $r ~ (r ≤ n)$개인 행렬 $C$ 완성!

행렬 $C$의 최종 $r$개의 열은 일차독립이며, 행렬 $A$의 열공간 $C(A)$의 기저(basis)입니다. 이때 행렬 $C$의 열의 개수(=행렬 $A$의 일차독립인 열의 개수) $r$은 행렬 $A$의 랭크, 행렬 $C$의 랭크입니다.

> **기저(Basis)**
> 
> 
> 벡터공간 $V$의 기저는
> 
> 1. 일차독립이고
> 2. 벡터공간 $V$를 span합니다.
> 
> > **Span**
> > 
> > 
> > 벡터 $v_1, v_2, \cdots, v_n$의 모든 일차결합으로 어떠한 벡터공간을 형성(span a space)합니다.
> > 
> 
> ⇒ 기저로 벡터공간 $V$의 모든 벡터를 표현할 수 있음
> 
> ⇒ **기저는 벡터공간 $V$를 만들기 위한 최소한의 벡터**
> 

> **랭크(Rank)**
> 
> - 행렬의 **일차독립인 열의 개수**
> - 행렬의 **열공간의 차원**

행렬 $C$에 포함되지 않은 행렬 $A$의 열은 행렬 $C$의 열의 일차결합입니다.


✏️ **예제 4**

$A =
\begin{bmatrix}
1 & 3 & 8 \\
1 & 2 & 6 \\
0 & 1 & 2
\end{bmatrix}$일 때, 행렬 $C$ 구하기

1. 행렬 $A$의 1열인 $\begin{bmatrix}
1 \\
1 \\
0
\end{bmatrix}$이 영벡터가 아니니, 행렬 $C$에 넣기
2. 행렬 $A$의 2열인 $\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}$이 1열의 상수배로 표현될 수 없으니, 행렬 $C$에 넣기
3. 행렬 $A$의 3열인 $\begin{bmatrix}
8 \\
6 \\
2
\end{bmatrix}$는 1열과 2열의 일차결합 $2
\begin{bmatrix}
1 \\
1 \\
0
\end{bmatrix} + 2
\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}$이니, 행렬 $C$에 넣지 않기

$\therefore C =
\begin{bmatrix}
1 & 3 \\
1 & 2 \\
0 & 1
\end{bmatrix}$, $r=2$

⇒ 열벡터 $\begin{bmatrix}
1 \\
1 \\
0
\end{bmatrix}$와 $\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}$는 행렬 $A$의 열공간 $C(A)$의 기저이고, 열공간 $C(A)$의 차원 $dim(C(A))$(= $rank(A)$)은 2입니다.




✏️ **예제 4**

$A =
\begin{bmatrix}
1 & 3 & 8 \\
1 & 2 & 6 \\
0 & 1 & 2
\end{bmatrix}$일 때, 행렬 $C$ 구하기

1. 행렬 $A$의 3열인 $\begin{bmatrix}
8 \\
6 \\
2
\end{bmatrix}$이 영벡터가 아니니, 행렬 $C$에 넣기
2. 행렬 $A$의 2열인 $\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}$이 1열의 상수배로 표현될 수 없으니, 행렬 $C$에 넣기
3. 행렬 $A$의 1열인 $\begin{bmatrix}
1 \\
1 \\
0
\end{bmatrix}$는 2열과 3열의 일차결합 $-
\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}
+ \frac{1}{2}
\begin{bmatrix}
8 \\
6 \\
2
\end{bmatrix}$이니, 행렬 $C$에 넣지 않기

$\therefore C =
\begin{bmatrix}
8 & 3 \\
6 & 2 \\
2 & 1
\end{bmatrix}$, $r=2$

⇒ 열벡터 $\begin{bmatrix}
8 \\
6 \\
2
\end{bmatrix}$와 $\begin{bmatrix}
3 \\
2 \\
1
\end{bmatrix}$는 행렬 $A$의 열공간 $C(A)$의 기저이고, 열공간 $C(A)$의 차원은 2입니다.

⚠️ 기저는 다양할 수 있지만, 랭크는 항상 일정합니다.

앞의 $A=CR$에서와 같이, 행렬 $A$를 행렬 $A$의 열공간 $C(A)$의 기저를 열로 하는 행렬 $C$와 행렬 $R$로 분해할 수 있습니다.

$$
A =
\begin{bmatrix}
1 & 3 & 8 \\
1 & 2 & 6 \\
0 & 1 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 3 \\
1 & 2 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 2 \\
0 & 1 & 2
\end{bmatrix}
= CR
$$

- 행렬 $A$의 1열: 행렬 $C$와 행렬 $R$의 1열 $\begin{bmatrix}
1 \\
0
\end{bmatrix}$의 곱
- 행렬 $A$의 2열: 행렬 $C$와 행렬 $R$의 2열 $\begin{bmatrix}0\\1\end{bmatrix}$의 곱
- 행렬 $A$의 3열: 행렬 $C$와 행렬 $R$의 3열 $\begin{bmatrix}2\\2\end{bmatrix}$의 곱

![열공간 관점의 A=CR](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/A=CR%201.png)

💡3. **행렬 $C$는 행렬 $A$의 열공간 $C(A)$의 기저**입니다.
**행렬 $R$의 각 열은 행렬 $A$의 각 열을 열공간의 기저의 일차결합으로 표현할 때의 상수**를 담고 있습니다.



![행공간 관점의 A=CR](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/A=CR%202.png)

💡4. **행렬 $R$은 행렬 $A$의 행공간의 기저**입니다.

**행렬 $C$의 각 행은 행렬 $A$의 각 행을 행공간의 기저의 일차결합으로 표현할 때의 상수**를 담고 있습니다.

💡5. **행공간의 차원과 열공간의 차원은 모두 랭크와 같습니다.**

⚠️ 한 공간을 구성하는 기저는 다양할 수 있습니다!

⚠️ 행렬 $A$의 행공간은 행렬 $A^T$의 열공간 $C(A^T)$와 같습니다.

- **해 존재 $\Leftrightarrow$ $b$가 행렬 $A$의 일차결합 $\Leftrightarrow$ $b \in C(A)$**

---

## 생각해보기

🤔 열공간이 의미하는 바는 무엇일까?

데이터와 weight matrix로 데이터의 representation을 만드는 과정을 생각해볼 수 있습니다. 이때, weight는 어떠한 데이터가 들어왔을 때 그 데이터를 weight의 열공간으로 보내는 매개체라고 볼 수 있습니다. 딥러닝에서 열공간은 데이터를 또다르게 표현하는 공간이고, weight는 그러한 공간이 데이터를 task에 맞게 잘 표현되는 공간이 되도록 업데이트됩니다.

![Attention](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/Attention.png)

> $x_{i}$의 query $q_{i}$는, $x_{i}$의 요소를 스칼라로 하는 $W^{Q}$의 열의 일차결합으로 표현됩니다.

✏️ **LoRA**

General tasks를 커버하는 큰 모델들은, 거의 full-rank인, size가 어마어마하게 큰 weight matrices로 이루어져 있는 경우가 많습니다. 그러한 큰 모델의 능력을 빌려 특정한 task를 잘하는 모델을 만들기 위해 tuning을 하는 것은 memory 측면에서 너무 비쌉니다.

LoRA는 특정 문제를 해결할 때 실제로는 weight의 rank보다 더 작은 차원만을 필요로 한다는 아이디어를 기반으로 합니다. 즉, **데이터를 더 작은 차원의 공간으로 보내도록 하더라도 충분히 그 공간에서 데이터를 잘 표현할 수 있습니다.**

LoRA에서는 (당연히 size도 크고) rank가 큰 $W$를 직접 학습하지 않습니다. 대신, (size가 훨씬 작고) rank가 훨씬 작은 행렬 $A$가 데이터를 $C(W)$보다 훨씬 더 작은 차원의 공간인 $C(A)$가 데이터를 잘 표현할 수 있는 방향으로 $A$를 업데이트합니다.

기존의 weight matrix $W_{0} \in \mathbb{R}^{d \times k}$, fine-tuning을 위한 weight matrix $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$ ($r \ll \min{(d, k)}$)가 있을 때, 데이터의 representation $h$는

$$h = W_{0} x + \Delta W_{x} = W_{0} x + BAx$$

와 같이 표현할 수 있습니다.

![LoRA 구조](/assets/images/[딥러닝을%20위한%20선형대수학]%201.1%20행렬%20A의%20열을%20이용한%20곱셈/LoRA.png)

> LoRA 구조. $A$는 입력 $x$를 rank가 (거의) $r$인 공간 $C(A)$로 보내고, $B$는 원래 weight $W$의 output과 같은 차원으로 다시 끌어올려 연산이 가능하도록 합니다.

LoRA는 이렇게 해서 기존 큰 모델이 많은 데이터를 잘 표현하도록 하는 능력도, 적당한 차원의 추가 공간으로 task에 특화되도록 데이터를 잘 표현하는 능력도 활용할 수 있도록 합니다.

